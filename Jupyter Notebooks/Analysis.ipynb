{
  "cells": [
    {
      "metadata": {
        "_uuid": "3a40fc4b131985c2afe08fa4ffc24cbf6a27a439",
        "_cell_guid": "6370bafc-5333-44b4-9846-e5ea66b49f44"
      },
      "cell_type": "markdown",
      "source": "# 1. Introduction"
    },
    {
      "metadata": {
        "_uuid": "958a7e69bebd9c38dcecacf5cb5fed8ed93c389e",
        "_cell_guid": "324cd6b0-79f1-4737-8050-6ab67f449046"
      },
      "cell_type": "markdown",
      "source": "This is my first kernel at Kaggle and my goal is to take it to completion through the stages of data exploration, data preprocessing, predicting and refinement. In the end I hope to get a rank on the leaderboard. \nI believe this is the best way to get started and I am thankful to everyone who has shared their work at Kaggle. "
    },
    {
      "metadata": {
        "_uuid": "09d829fbc0e1571d2247046259fb8481e12511d5",
        "_cell_guid": "855b31e7-b0ca-4092-8b4a-b103e55861aa",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Loading the relevant Python modules\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import Imputer\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c8768e48efc4710a494cdea8142b5f70b3d0433b",
        "_cell_guid": "e2737b26-db1b-4ad9-8a17-fa8e7aa9f819"
      },
      "cell_type": "markdown",
      "source": "# 2. Analysis"
    },
    {
      "metadata": {
        "_uuid": "e669960110c86a0b5e2e6be5278f7f896ec1950c",
        "_cell_guid": "980c979c-0aef-454c-888d-e22cf7fe4c36"
      },
      "cell_type": "markdown",
      "source": "## Data Exploration"
    },
    {
      "metadata": {
        "_uuid": "e76de66ba03f47d004b82c2d1e908ce09912555e",
        "_cell_guid": "5b861ee7-2e48-4442-8af6-6288b0fb4216"
      },
      "cell_type": "markdown",
      "source": "Making use of EDA done by [Anisotropic](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)"
    },
    {
      "metadata": {
        "_uuid": "b9b19da712fb64815d8e69edc3b8521d580c7b57",
        "_cell_guid": "70d8a7e9-e05c-45d1-98a6-8c4d38c1428f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Importing the training and testing data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n# Printing first 5 rows of training data\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "38061345a3e7a2aabc15cd157764f8f5646ae675",
        "_cell_guid": "b78a19c2-c4f9-4740-811e-ab5a0b9dd5ca",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Taking a look at rows and columns of the train and test dataset\nrows_train = train.shape[0]\ncolumns_train = train.shape[1]\nprint(\"The train dataset contains {0} rows and {1} columns\".format(rows_train, columns_train))\nrows_test = test.shape[0]\ncolumns_test = test.shape[1]\nprint(\"The test dataset contains {0} rows and {1} columns\".format(rows_test, columns_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5bba1e0d9ecd86f8ce2c44080ed5fe08d501662a",
        "_cell_guid": "e7f737a4-5c0b-4c4f-82f8-b03e71cd1a2a"
      },
      "cell_type": "markdown",
      "source": "So, we can see that there are total 595212 rows and 59 columns in training data. As one column is for ID and one is for the target value, there are total 57 features in the training data. Along the same lines the test data set contains 892816 rows and 58 columns. Since, we have to predict target value there is one column less.\n\n2 important points that I observed in the data description of the competition were that:\n1. Null values have been replaced by -1\n2. Column names just give an idea of type of variable and not much information is provided about what the feature actually means.\n\nSo, even though checking for null values in data would return nothing, we have to take care of them. Also, we need to make use of various statistical and computational methods in order to derive meaning from the data."
    },
    {
      "metadata": {
        "_uuid": "4aefa83dbb66987ae5ece3db156e409e67ef66e6",
        "_cell_guid": "b42535b2-4ee0-4f39-b2d8-b36b3c61f001",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Checking for null values\ntrain.isnull().any().any()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acd6626f128faf266ab430798d807144376a103a",
        "_cell_guid": "c5b035c6-b0f6-44c9-b153-89a9b1da9549"
      },
      "cell_type": "markdown",
      "source": "### **Null or missing values**"
    },
    {
      "metadata": {
        "_uuid": "8a0b3299cb79926ce13d9d0eb0487226fb873d95",
        "_cell_guid": "60e58938-8996-48a6-a977-82e329249870",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import missingno as msno\n#Creating a copy of training data\ntrain_null = train\ntrain_null = train_null.replace(-1, np.NaN)\n\nmsno.matrix(df=train_null.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "185ae7331dfe0558d24252848137af5fc55071f1",
        "_cell_guid": "3b46297d-9509-457e-8944-6a0cfc77f105"
      },
      "cell_type": "markdown",
      "source": "In this visualization missing data is shown by white bands superimposed on dark red bands that show the values that are not missing or null. This visualization is really helpful in order to get a visual estimate of amount of data that is missing and to clearly find out which features have the most missing values. However, this visualization excludes certain null features as it can only fit in approximately 40 odd features. So, from the visualization we can see that ps_reg_03, ps_car_03_cat and ps_car_05_cat have the most missing values and we can get the list of features with missing values as below."
    },
    {
      "metadata": {
        "_uuid": "cbc434fd9ac3f142212451201b4706b699c5e354",
        "_cell_guid": "9521cf3f-ce68-459e-b814-eacc4f6fcf3d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test_null = test\ntest_null = test_null.replace(-1, np.NaN)\n# Extract columns with null data\ntrain_null = train_null.loc[:, train_null.isnull().any()]\ntest_null = test_null.loc[:, test_null.isnull().any()]\n\nprint(train_null.columns)\nprint(test_null.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81faf59d5265916a7da51349d3768d7d3f822102",
        "_cell_guid": "f38cc62a-181a-4fbd-86bd-645f3d8fbb8d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('Columns \\t Number of NaN')\nfor column in train_null.columns:\n    print('{}:\\t {}'.format(column,len(train_null[column][np.isnan(train_null[column])])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ca344cd2994dd3a50cbab72b0a86908fe5ed2c1",
        "_cell_guid": "02530b5e-b53b-4c26-a2f0-079821b0a910"
      },
      "cell_type": "markdown",
      "source": "**ps_car_03_cat**, **ps_car_05_cat**, **ps_reg_03** , **ps_car_14** and **ps_car_07_cat** have missing values for more than 10,000 rows in training data. Many null values will cause error in training of models and which will lead to worng predictions. We will need to do feature analysis in order to decide a strategy for treating these missing values."
    },
    {
      "metadata": {
        "_uuid": "840d010635b3561bd879438987e7e112857f2a53",
        "_cell_guid": "7c980abf-ae2f-4858-81af-4887668d0409"
      },
      "cell_type": "markdown",
      "source": "### **Target Exploration**"
    },
    {
      "metadata": {
        "_uuid": "0625c6b337ed8a0462b71e7919cf76e10b5e3d22",
        "_cell_guid": "7c96147a-4fac-41fd-86f5-2001289f7f4f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "targets = train['target'].values\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x = targets)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(targets)), (p.get_x()+ 0.3, p.get_height()+10000))\nplt.title('Distribution of Target', fontsize=20)\nplt.xlabel('Claim', fontsize=20)\nplt.ylabel('Frequency [%]', fontsize=20)\nax.set_ylim(top=700000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d72b90ee8d11f7a083e9f553e181143f6f559e5",
        "_cell_guid": "df73f021-8e61-434c-a86c-9a3e4fe62070"
      },
      "cell_type": "markdown",
      "source": "As we can see, target value is highly imbalanced. Value for target is 1 for 3.64% of the records in training data. So, if we use a naive classifier that simply classifies target as 0 for all the rows then the prediction accuracy will be very high. Also, if we train a model using such imbalanced data then it will have high accuracy as the model will be biased to label data as 1 regardless of the data it is asked to predict. We will have to use a [strategy](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/) to overcome this problem."
    },
    {
      "metadata": {
        "_uuid": "2211c6c27d92f76a4b4244729d897363813851c4",
        "_cell_guid": "47758761-7a76-454d-9a21-dfa7f46483d8"
      },
      "cell_type": "markdown",
      "source": "## Exploratory Visualization"
    },
    {
      "metadata": {
        "_uuid": "2000c483e68b41b9f26c84de40ccb97f2c6e7b85",
        "_cell_guid": "ec33a211-120d-46ad-97f0-5a123cf78ede"
      },
      "cell_type": "markdown",
      "source": "### Datatype check"
    },
    {
      "metadata": {
        "_uuid": "b2a8e90e7ad49b162d87aff666c7ce6bfd6db00e",
        "_cell_guid": "6558d0d9-a92b-40dc-a810-2a2c422dcb5c"
      },
      "cell_type": "markdown",
      "source": "In order to visualize the data it would be a good idea to check kind of datatypes the training data is made up of. Based on the trick provided by   we can get the count of kind of datatype as follows:"
    },
    {
      "metadata": {
        "_uuid": "b5af6cc0368b97d218265713503306b53a1bd230",
        "_cell_guid": "fd72f48f-5c74-46c5-bc13-f4d60a01c248",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Counter(train.dtypes.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "917246cec5f589ed5c448863579bac5b52aedbd9",
        "_cell_guid": "dd0db1c7-2455-4e60-9800-2e17962cbf23"
      },
      "cell_type": "markdown",
      "source": "So, there are only 2 datatypes int and float in the training data. So, we can divide the traiing data into 2 parts:"
    },
    {
      "metadata": {
        "_uuid": "433efe983be47dcece3435388b9299a7570086ee",
        "_cell_guid": "9ea51a9d-7a5b-4a3e-8d76-54cc9b031338",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d414b1ffc28aadbc1e0582d863ff640c5979be85",
        "_cell_guid": "1989e9d2-1737-483c-93e4-47de7297e6c1"
      },
      "cell_type": "markdown",
      "source": "Data provided by Porto Seguro has suffixes with abbreviations such as \"bin\", \"cat\" and \"reg\", where bin  indicates binary features while cat indicates categorical features while the rest are either continuous or ordinal features. So, train_float has continuous features whereas train_int has binary, categorical and ordinal features."
    },
    {
      "metadata": {
        "_uuid": "6faf6af94fb854e66e0a4c37e17a1a82c5190a8c",
        "_cell_guid": "a0fb19b1-ccf0-4661-ac21-a1ad14e14673"
      },
      "cell_type": "markdown",
      "source": "### Metadata"
    },
    {
      "metadata": {
        "_uuid": "63c9615dc26f007b955904f7d0a7b74a32b57c1b",
        "_cell_guid": "85b9ea2a-9a3e-47d2-aea9-c31468f2669e"
      },
      "cell_type": "markdown",
      "source": "I really liked the idea of metadata as mentioned by <>. So, we can create a dataframe to store metadata for training data such as:\n* **role**: input, ID, target\n* **level**: nominal, interval, ordinal, binary\n* **keep**: True or False\n* **dtype**: int, float, str"
    },
    {
      "metadata": {
        "_uuid": "c8e8a298bd23376a842a827a80cb1e3e39b2523e",
        "_cell_guid": "b32027fb-13f2-460e-a367-e46ce27a12cc",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "data = []\nfor f in train.columns:\n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # Defining the data type \n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acd00ca5d1bf3e08ccc79e4b53801177f9e7ce98",
        "_cell_guid": "c00def93-41c1-4e57-afeb-1de536318d35"
      },
      "cell_type": "markdown",
      "source": "So, we can get a nice summary of meta data as below:"
    },
    {
      "metadata": {
        "_uuid": "f923d801e1f668cd66a9a5e32de4c59fb33fd365",
        "_cell_guid": "b4539a35-a86c-4c40-b606-9d77160f5c8b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c10a4a1c97419ea78f43cc4c3c60346b90bea93d",
        "_cell_guid": "365c879a-da53-4b28-908d-900f3d89329b"
      },
      "cell_type": "markdown",
      "source": "### **Correlation Plots**"
    },
    {
      "metadata": {
        "_uuid": "fd6560f9048a6b5966836434886290156fcf748e",
        "_cell_guid": "4a8f7adb-7651-453d-93e0-57cd7dd0c3e9"
      },
      "cell_type": "markdown",
      "source": "Correlation plots are useful to check if there is a high correlation between variables or not. This will be useful in deciding if PCA would be helpful or not."
    },
    {
      "metadata": {
        "_uuid": "fd72514447d55168123a2fae027dbea8e0d1e958",
        "_cell_guid": "947b2983-2fa8-4fb9-923f-ee23f3413f90"
      },
      "cell_type": "markdown",
      "source": "#### **Correlation of float features**"
    },
    {
      "metadata": {
        "_uuid": "5cfe6a11e25aad3c1921d0c8526a792e9337ae62",
        "_cell_guid": "2ce03c42-09fb-4104-9495-968a9cea62d6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "colormap = plt.cm.magma\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd1d838efb0fb55e223fe218587c9fc247281815",
        "_cell_guid": "1e18f97b-b43e-4637-9d65-7aa39ae945ab"
      },
      "cell_type": "markdown",
      "source": "From the correlation plot, we can see that the majority of the features display zero or no correlation to one another.  Ony paired features such as (ps_reg_01, ps_reg_03), (ps_reg_02, ps_reg_03), (ps_car_12, ps_car_13), (ps_car_13, ps_car_15) have high correlation. So, it might not make any difference if we do PCA on the training data as the number of correlated variables is low."
    },
    {
      "metadata": {
        "_uuid": "ae768cc0c2105d6be6f1b341147cdc0c5c3a3e84",
        "_cell_guid": "9bba8298-8317-4440-8387-7c1955353bf9"
      },
      "cell_type": "markdown",
      "source": "#### **Correlation of integer features**"
    },
    {
      "metadata": {
        "_uuid": "bbc0054b5c6542c74ccfbbf466b7b9fd3f2e4bd7",
        "_cell_guid": "5ab22039-64a2-4e74-a2ee-e0673887213f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data = [\n    go.Heatmap(\n        z= train_int.corr().values,\n        x=train_int.columns.values,\n        y=train_int.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        text = True ,\n        opacity = 1.0 )\n]\n\nlayout = go.Layout(\n    title='Pearson Correlation of Integer-type features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "50ba64e9e563b2410e5356bdf452eef6cefb62e9",
        "_cell_guid": "57dfc327-2733-40f4-8def-0caa80540c42"
      },
      "cell_type": "markdown",
      "source": "The heat map show above uses plot.ly as descirbed by <>. x and y axes take in the column names while the correlation value is provided by the z-axis. It is quite evident that a huge number of columns with integer datatype are also not linearly correlated."
    },
    {
      "metadata": {
        "_uuid": "4635f8ff70ad2ade03eb75b1f68f90b9347d6b75",
        "_cell_guid": "3828b3c9-5e1b-4f61-85ec-2dd373c1a3f4"
      },
      "cell_type": "markdown",
      "source": "### **Binary features inspection**"
    },
    {
      "metadata": {
        "_uuid": "c224881091893842f47f183bf774faba581649c6",
        "_cell_guid": "db5d8cac-8c3e-4c3f-b49b-4e1fbd1fb5fb",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum())\n    one_list.append((train[col]==1).sum())\n    \ntrace1 = go.Bar(\n    x=bin_col,\n    y=zero_list ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=bin_col,\n    y=one_list,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "67219cd1b77380fada57e863ef8c98d3cadc26c4",
        "_cell_guid": "dd06ee7c-2a5b-44e7-8beb-9c9e0969fb5f"
      },
      "cell_type": "markdown",
      "source": "From this plot we can observe that **ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin** are completely dominated by zeros and they will be of no use in our prediction model."
    },
    {
      "metadata": {
        "_uuid": "5c39042594d9980fcfb7e21e60481ac0edaa88c8",
        "_cell_guid": "c0b9f28e-8b73-4bcf-8503-eadfc063402d"
      },
      "cell_type": "markdown",
      "source": "### **Categorical and Ordinal Feature Inspection**"
    },
    {
      "metadata": {
        "_uuid": "83f4b178785ffdd5754c7cbb9db7975225fc615b",
        "_cell_guid": "309ace64-d230-4f75-ae8c-107163c7d6eb"
      },
      "cell_type": "markdown",
      "source": "#### **Checking the cardinality of the categorical variables**\n\nCardinality refers to the number of different values in a variable. As we will create dummy variables from the categorical variables later on, we need to check whether there are variables with many distinct values. We should handle these variables differently as they would result in many dummy variables."
    },
    {
      "metadata": {
        "_uuid": "9489f4c72627ba04479e24280c0b36e7bc560bf7",
        "_cell_guid": "9f6efa24-eaf1-47b9-a2b7-5de12501cd3d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e86476e940452935fff277e6996be7c228171e2d",
        "_cell_guid": "59173401-1ab0-48df-b36f-8558b3a29ef1"
      },
      "cell_type": "markdown",
      "source": "Only ps_car_11_cat has many distinct values, although it is still reasonable."
    },
    {
      "metadata": {
        "_uuid": "f69e22fea6cc0ca8714f57a874a5b561dd02def4",
        "_cell_guid": "8ceb05e0-9e5c-497d-a43e-9552b2965aca"
      },
      "cell_type": "markdown",
      "source": "#### **Feature Importance**"
    },
    {
      "metadata": {
        "_uuid": "d5ba2050cc28dffd44f284efdad13da9b119c84d",
        "_cell_guid": "e420d963-3724-4d58-bcc5-1e22fd522fb1"
      },
      "cell_type": "markdown",
      "source": "We can obtain feature importance using RandomForestClassifier of sklearn. Having trained the Random Forest, we can obtain the list of feature importances by invoking the attribute \"featureimportances\" and display a sorted list of all the features ranked by order of their importance, from highest to lowest via the same plotly barplots as follows"
    },
    {
      "metadata": {
        "_uuid": "c56f3acafa71fb33256b63cd9038f89a27105563",
        "_cell_guid": "d5f06e3e-540f-4762-82bd-d66ec22de267",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nrf.fit(train.drop(['id', 'target'],axis=1), train.target)\nfeatures = train.drop(['id', 'target'],axis=1).columns.values\nprint(\"----- Training Done -----\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "abd2c33a7fae59245648cbfbb1b762f7c31d0403",
        "_cell_guid": "0a1707c6-0d75-4dbb-9767-b7721af37335",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Random Forest Feature importance',\n    orientation='h',\n)\n\nlayout = go.Layout(\n    width = 900, height = 2000,\n    title='Barplot of Feature importances',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ))\n\nfig1 = go.Figure(data=[trace2], layout = layout)\n\npy.iplot(fig1, filename='plots')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.6.3",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}